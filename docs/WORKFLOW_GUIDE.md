# Data Processing Workflow Guide

HÆ°á»›ng dáº«n chi tiáº¿t quy trÃ¬nh xá»­ lÃ½ data cho dá»± Ã¡n AML Detection vá»›i constraint 4GB RAM.

---

## ğŸ¯ Tá»•ng quan

Quy trÃ¬nh gá»“m **4 phases**, má»—i phase cÃ³ thá»ƒ cháº¡y Ä‘á»™c láº­p hoáº·c liÃªn tiáº¿p. Táº¥t cáº£ phases Ä‘á»u tá»‘i Æ°u cho 4GB RAM.

**Thá»© tá»± thá»±c hiá»‡n**: Phase 1 â†’ Phase 2 â†’ Phase 3 â†’ Phase 4

---

## ğŸ“‹ Phase 1: Build Index

**Thá»i gian**: 5-10 phÃºt  
**RAM**: ~100 MB  
**Disk I/O**: Low

### Má»¥c tiÃªu
Táº¡o cÃ¡c lookup tables Ä‘á»ƒ tra cá»©u nhanh, trÃ¡nh Ä‘á»c láº¡i CSV lá»›n nhiá»u láº§n.

### Input
- `data/raw/nodes.csv` (7.2 MB)
- `data/raw/edges.csv` (11 MB)
- `data/raw/connected_components.csv` (1.6 MB)

### Output
```
data/processed/index/
â”œâ”€â”€ node_to_idx.pkl          # Dict: clId â†’ index
â”œâ”€â”€ idx_to_node.pkl          # Dict: index â†’ clId
â”œâ”€â”€ node_labels.pkl          # Dict: clId â†’ label (0/1)
â”œâ”€â”€ component_to_nodes.pkl   # Dict: ccId â†’ [clIds]
â””â”€â”€ edges_index.pkl          # Adjacency list
```

### Cháº¡y
```bash
python src/data_processing/phase1_build_index.py
```

### Kiá»ƒm tra
```bash
ls -lh data/processed/index/
```

---

## ğŸ“‹ Phase 2: Extract Features (QUAN TRá»ŒNG NHáº¤T)

**Thá»I gian**: 2-3 giá»  
**RAM**: ~200 MB  
**Disk I/O**: VERY HIGH (Ä‘á»c 78GB, ghi 2-3GB)

### Má»¥c tiÃªu
TrÃ­ch xuáº¥t 95 features tá»« `background_edges.csv` (78GB) vÃ  lÆ°u thÃ nh individual files.

### Challenge
File 78GB khÃ´ng thá»ƒ load vÃ o RAM 4GB.

### Solution
**Streaming vá»›i chunks**:
- Äá»c 50,000 rows má»—i láº§n (~50-100 MB RAM)
- Filter chá»‰ giá»¯ edges liÃªn quan Ä‘áº¿n 444k nodes
- Extract vÃ  lÆ°u ngay ra disk
- XÃ³a chunk khá»i RAM

### Input
- `data/raw/background_edges.csv` (78 GB)
- `data/processed/index/node_to_idx.pkl`

### Output
```
data/processed/features/
â”œâ”€â”€ node_12345_in.csv       # Incoming transactions
â”œâ”€â”€ node_12345_out.csv      # Outgoing transactions
â”œâ”€â”€ ... (444k files)
```

**Format má»—i file**:
```csv
txId,feat#1,feat#2,...,feat#95,timestamp_proxy
50679415,40,68,...,51,0.456
589133991,53,68,...,59,0.789
```

### Cháº¡y
```bash
# Láº§n Ä‘áº§u
python src/data_processing/phase2_extract_features.py

# Hoáº·c resume náº¿u bá»‹ giÃ¡n Ä‘oáº¡n
python src/data_processing/phase2_extract_features.py --resume
```

### Theo dÃµi tiáº¿n Ä‘á»™
```bash
# Terminal 1: Cháº¡y processing
python src/data_processing/phase2_extract_features.py

# Terminal 2: Monitor RAM
./scripts/monitor_ram.sh

# Terminal 3: Xem sá»‘ files Ä‘Ã£ táº¡o
watch -n 10 'ls data/processed/features/ | wc -l'
```

### Checkpointing
- Tá»± Ä‘á»™ng lÆ°u checkpoint má»—i 1000 chunks
- Checkpoint file: `checkpoints/phase2_checkpoint.pkl`
- CÃ³ thá»ƒ resume tá»« checkpoint báº±ng flag `--resume`

---

## ğŸ“‹ Phase 3: Build Sequences

**Thá»I gian**: 30-60 phÃºt  
**RAM**: ~100 MB  
**Disk I/O**: Medium

### Má»¥c tiÃªu
XÃ¢y dá»±ng temporal sequences (in-flow vÃ  out-flow) cho tá»«ng node.

### Input
- `data/processed/features/` (2-3 GB)
- `data/processed/index/`

### Process
1. Äá»c `node_{clId}_in.csv` vÃ  `node_{clId}_out.csv`
2. Sort by txId (ascending) - thá»© tá»± thá»I gian
3. Giá»¯ K=50 transactions gáº§n nháº¥t
4. Padding náº¿u thiáº¿u
5. LÆ°u thÃ nh numpy array

### Output
```
data/processed/sequences/
â”œâ”€â”€ node_000000.npz          # Shape: (2, 50, 97)
â”œâ”€â”€ node_000001.npz          #   Dim 0: [in_flow, out_flow]
â”œâ”€â”€ ...                      #   Dim 1: 50 transactions
â””â”€â”€ metadata.json            #   Dim 2: 97 features
```

### Cháº¡y
```bash
python src/data_processing/phase3_build_sequences.py
```

### Kiá»ƒm tra
```python
import numpy as np

# Load 1 sample
data = np.load('data/processed/sequences/node_000000.npz')
print(data['in_flow'].shape)   # (50, 97)
print(data['out_flow'].shape)  # (50, 97)
```

---

## ğŸ“‹ Phase 4: Build Graph

**Thá»I gian**: 5-10 phÃºt  
**RAM**: ~100 MB  
**Disk I/O**: Low

### Má»¥c tiÃªu
Táº¡o graph structure cho GNN training.

### Input
- `data/raw/edges.csv` (11 MB)
- `data/processed/index/`

### Output
```
data/processed/graph/
â”œâ”€â”€ edge_index.pt            # torch.Tensor [2, num_edges]
â”œâ”€â”€ edge_attr.pt             # torch.Tensor [num_edges, 3]
â”œâ”€â”€ adjacency_list.pkl       # Dict: node_idx â†’ [neighbors]
â””â”€â”€ train_val_test_split.pkl # Indices for splits
```

### Train/Val/Test Split
- **Train**: 70% (stratified)
- **Val**: 15% (stratified)
- **Test**: 15% (stratified)
- Giá»¯ nguyÃªn tá»· lá»‡ class imbalance (97.7:2.3)

### Cháº¡y
```bash
python src/data_processing/phase4_build_graph.py
```

---

## ğŸš€ CÃ¡ch Cháº¡y

### Option 1: Cháº¡y tá»«ng phase riÃªng láº» (Khuyáº¿n nghá»‹)

```bash
cd aml_project

# Phase 1
python src/data_processing/phase1_build_index.py

# Phase 2 (cÃ³ thá»ƒ pause/resume)
python src/data_processing/phase2_extract_features.py

# Náº¿u bá»‹ giÃ¡n Ä‘oáº¡n, resume
python src/data_processing/phase2_extract_features.py --resume

# Phase 3
python src/data_processing/phase3_build_sequences.py

# Phase 4
python src/data_processing/phase4_build_graph.py
```

### Option 2: Cháº¡y táº¥t cáº£ báº±ng script

```bash
cd aml_project
./scripts/run_processing.sh
```

Script nÃ y sáº½ há»i báº¡n muá»‘n cháº¡y:
1. All phases
2. Specific phase
3. Resume from checkpoint

### Option 3: Monitor RAM trong lÃºc cháº¡y

```bash
# Terminal 1: Start processing
python src/data_processing/phase2_extract_features.py

# Terminal 2: Monitor RAM
./scripts/monitor_ram.sh

# Logs will be saved to: logs/ram_usage.log
```

---

## âš ï¸ LÆ°u Ã Quan Trá»ng

### 1. Phase 2 cÃ³ thá»ƒ bá»‹ giÃ¡n Ä‘oáº¡n
- Máº¥t Ä‘iá»‡n, crash, hoáº·c muá»‘n dá»«ng nghá»‰
- **LuÃ´n dÃ¹ng checkpoint**: `python phase2_extract_features.py --resume`

### 2. Disk space
- Phase 2 táº¡o ~444k files â†’ cáº§n filesystem há»— trá»£ nhiá»u files
- Náº¿u gáº·p lá»—i "too many files", cÃ³ thá»ƒ dÃ¹ng SQLite thay vÃ¬ CSV files

### 3. Thá»I gian
- Phase 2 lÃ  bottleneck: 2-3 giá»
- CÃ³ thá»ƒ cháº¡y overnight
- CÃ¡c phases khÃ¡c ráº¥t nhanh

### 4. Backup
- Sau Phase 2, nÃªn backup folder `data/processed/features/`
- Náº¿u máº¥t, pháº£i cháº¡y láº¡i 2-3 giá»

---

## ğŸ“Š Expected Results

Sau khi hoÃ n thÃ nh 4 phases:

```
data/processed/
â”œâ”€â”€ index/          ~10 MB
â”œâ”€â”€ features/       ~2-3 GB (444k files)
â”œâ”€â”€ sequences/      ~3-4 GB (444k files)
â””â”€â”€ graph/          ~100 MB
```

**Total**: ~5-8 GB processed data

---

## ğŸ”§ Troubleshooting

### Issue: "Killed" hoáº·c crash trong Phase 2
**NguyÃªn nhÃ¢n**: RAM háº¿t  
**Giáº£i phÃ¡p**:
- Giáº£m CHUNK_SIZE trong config.py (50k â†’ 25k)
- TÄƒng swap space
- ÄÃ³ng cÃ¡c á»©ng dá»¥ng khÃ¡c

### Issue: "Too many open files"
**NguyÃªn nhÃ¢n**: OS limit  
**Giáº£i phÃ¡p**:
```bash
ulimit -n 65536  # TÄƒng limit
```

### Issue: Phase 2 cháº­m quÃ¡
**NguyÃªn nhÃ¢n**: Disk I/O bottleneck  
**Giáº£i phÃ¡p**:
- DÃ¹ng SSD thay vÃ¬ HDD
- Hoáº·c chuyá»ƒn sang SQLite (tÃ´i cÃ³ thá»ƒ implement náº¿u cáº§n)

### Issue: Phase 3/4 khÃ´ng tÃ¬m tháº¥y files
**NguyÃªn nhÃ¢n**: Phase trÆ°á»›c chÆ°a hoÃ n thÃ nh  
**Giáº£i phÃ¡p**: Kiá»ƒm tra logs vÃ  cháº¡y láº¡i phase trÆ°á»›c

---

## âœ… Verification Checklist

Sau má»—i phase, kiá»ƒm tra:

- [ ] Phase 1: `ls data/processed/index/` cÃ³ 5 files
- [ ] Phase 2: `ls data/processed/features/ | wc -l` â‰ˆ 888k (444k Ã— 2)
- [ ] Phase 3: `ls data/processed/sequences/ | wc -l` â‰ˆ 444k
- [ ] Phase 4: `ls data/processed/graph/` cÃ³ 4 files

---

## ğŸ“ Next Steps

Sau khi hoÃ n thÃ nh data processing
1. Review data quality (EDA trong notebooks/)
2. Báº¯t Ä‘áº§u implement model (LAS-Mamba-GNN)
3. Training vÃ  evaluation


